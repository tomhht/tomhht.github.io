<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://tomhht.github.io</id>
    <title>hello, world</title>
    <updated>2020-09-23T03:23:57.243Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://tomhht.github.io"/>
    <link rel="self" href="https://tomhht.github.io/atom.xml"/>
    <logo>https://tomhht.github.io/images/avatar.png</logo>
    <icon>https://tomhht.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, hello, world</rights>
    <entry>
        <title type="html"><![CDATA[OpenStack安装调试记录]]></title>
        <id>https://tomhht.github.io/post/openstack-an-zhuang-diao-shi-ji-lu/</id>
        <link href="https://tomhht.github.io/post/openstack-an-zhuang-diao-shi-ji-lu/">
        </link>
        <updated>2020-09-20T06:39:47.000Z</updated>
        <summary type="html"><![CDATA[<p>非生产环境，仅学习用。</p>
]]></summary>
        <content type="html"><![CDATA[<p>非生产环境，仅学习用。</p>
<!-- more -->
<h1 id="实验环境">实验环境</h1>
<ol>
<li>Ubuntu Server 20.04 LTS</li>
<li>Ubuntu 20.04 LTS</li>
</ol>
<h1 id="安装devstack">安装DevStack</h1>
<h2 id="创建用户-stack">创建用户 stack</h2>
<pre><code>sudo useradd -s /bin/bash -d /opt/stack -m stack
echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack
sudo su - stack
</code></pre>
<h2 id="下载devstack">下载DevStack</h2>
<pre><code>git clone https://github.com/openstack/devstack
cd devstack
</code></pre>
<h2 id="创建localconf文件">创建local.conf文件</h2>
<pre><code>cat &lt;&lt;EOF &gt; local.conf
[[local|localrc]]
ADMIN_PASSWORD=secret
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
Q_PLUGIN=ml2
ENABLE_TENANT_VLANS=True
ML2_VLAN_RANGES=physnet1:1000:2000
GIT_BASE=http://git.trystack.cn
NOVNC_REPO=http://git.trystack.cn/kanaka/noVNC.git
SPICE_REPO=http://git.trystack.cn/git/spice/spice-html5.git
</code></pre>
<!-- ## 修改stackrc里的repository
`stackrc`里面有后面安装要用到的`git clone`的仓库地址，默认的`opendev.org`特别慢，换成`github`好很多。
```
sed -i 's/opendev\.org/github\.com/' /opt/stack/devstack/stackrc
``` -->
<h2 id="修改pypi国内镜像">修改pypi国内镜像</h2>
<pre><code>pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
</code></pre>
<h2 id="开始安装">开始安装</h2>
<pre><code>./stack.sh
</code></pre>
<h3 id="troubleshooting">Troubleshooting</h3>
<!-- 1. 遇到从`opendev.org`下载超时的情况，换为`github`地址下载：
```
git clone https://github.com/openstack/requirements.git /opt/stack/requirements --branch master
git clone https://github.com/openstack/keystone.git /opt/stack/keystone --branch master
git clone https://github.com/openstack/glance.git /opt/stack/glance --branch master
git clone https://github.com/openstack/cinder.git /opt/stack/cinder --branch master
git clone https://github.com/openstack/nova.git /opt/stack/nova --branch master
git clone https://github.com/openstack/neutron.git /opt/stack/neutron --branch master
git clone https://github.com/openstack/horizon.git /opt/stack/horizon --branch master
git clone https://github.com/openstack/swift.git /opt/stack/swift --branch master
``` -->
<ol>
<li><code>github etcd</code>无法下载，走梯子。</li>
</ol>
<pre><code>curl -x http://192.168.127.1:58591 -fsSL https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz -o /opt/stack/devstack/files/etcd-v3.3.12-linux-amd64.tar.gz
</code></pre>
<ol start="2">
<li>报错<code>table 'broute' is incompatible, use 'nft' tool</code></li>
</ol>
<pre><code>update-alternatives --set ebtables /usr/sbin/ebtables-legacy || true
</code></pre>
<p>据说是Ubuntu 20.04才会出现的问题，18.04无此问题。感谢<a href="https://www.journaldev.com/30037/install-openstack-ubuntu-devstack">How to Install OpenStack on Ubuntu 18.04 with DevStack</a>文后的回复者Gyan。<br>
3. 反复遇到<code>Failure creating NET_ID for private</code>问题<br>
彻底卸了再多装几次。</p>
<pre><code>./unstack.sh
./clean.sh
./stack.sh
</code></pre>
<p>之前google baidu bing了n多，该有的<code>ML2_VLAN_RANGES=physnet1:1000:2000</code>、<code>vni_ranges = 1:1000</code>之类也都搞了，再安装依然报同样错。<strong>特别感谢</strong><a href="https://my.oschina.net/haitaohu/blog/3084104">通过devstack，在Vmware中使用Centos7快速安装体验openstack（单节点安装）</a>作者提到的卸载再装的方法。</p>
<blockquote>
<p>推测其他一些难解的问题也可以尝试。</p>
</blockquote>
<!-- 3. pip版本不够新，遇到下面警示后紧接着就报错，解决方法就是用提示
问题：
```
WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.
You should consider upgrading via the '/opt/stack/tempest/.tox/tempest/bin/python -m pip install --upgrade pip' command.
```
解决：
```
/opt/stack/tempest/.tox/tempest/bin/python -m pip install --upgrade pip
``` -->
<h2 id="成功的样子">成功的样子！</h2>
<pre><code>=========================
DevStack Component Timing
 (times are in seconds)  
=========================
wait_for_service      26
pip_install          136
apt-get                8
run_process           72
dbsync                45
apt-get-update         3
test_with_retry        3
osc                  196
-------------------------
Unaccounted time     657
=========================
Total runtime        1146



This is your host IP address: 192.168.0.109
This is your host IPv6 address: ::1
Horizon is now available at http://192.168.0.109/dashboard
Keystone is serving at http://192.168.0.109/identity/
The default users are: admin and demo
The password: secret

Services are running under systemd unit files.
For more information see: 
https://docs.openstack.org/devstack/latest/systemd.html

DevStack Version: victoria
Change: 1f8109ac29c6222fea2f02ffd487701de29e2355 Merge &quot;Further py2 cleanup for Fedora&quot; 2020-09-19 11:36:12 +0000
OS Version: Ubuntu 20.04 focal

2020-09-23 02:51:43.021 | stack.sh completed in 1148 seconds.
</code></pre>
<hr>
<p>参考链接：<br>
<a href="https://docs.openstack.org/devstack/latest/">DevStack</a><br>
<a href="https://yangsijie666.github.io/2018/09/12/devstack%E5%AE%89%E8%A3%85R%E7%89%88/">openstack学习之devstack安装</a><br>
<a href="https://www.yunforum.net/group-topic-id-1561.html">devstack安装使用openstack常见问题与解决办法</a><br>
<a href="https://www.journaldev.com/30037/install-openstack-ubuntu-devstack">How to Install OpenStack on Ubuntu 18.04 with DevStack</a><br>
<a href="https://my.oschina.net/haitaohu/blog/3084104">通过devstack，在Vmware中使用Centos7快速安装体验openstack（单节点安装）</a><br>
<a href="http://git.trystack.cn/cgit">TryStack Git Mirror</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[干掉Win10顽固不化的某快速访问固定项]]></title>
        <id>https://tomhht.github.io/post/gan-diao-win10-wan-gu-bu-hua-de-mou-kuai-su-fang-wen-gu-ding-xiang/</id>
        <link href="https://tomhht.github.io/post/gan-diao-win10-wan-gu-bu-hua-de-mou-kuai-su-fang-wen-gu-ding-xiang/">
        </link>
        <updated>2020-09-15T00:59:03.000Z</updated>
        <summary type="html"><![CDATA[<p>快速访问里有个Documents死活无法取消固定，点了又说不存在，让删又删不了，lese。</p>
]]></summary>
        <content type="html"><![CDATA[<p>快速访问里有个Documents死活无法取消固定，点了又说不存在，让删又删不了，lese。</p>
<!-- more -->
<p>感谢参考链接的作者，该方法效若桴鼓。</p>
<blockquote>
<p>注意事项：该方法会删除快速访问里的所有内容，考虑清楚再动手。（我的本来就只有我的电脑和烦人的那项，所以果断kill）</p>
</blockquote>
<ol>
<li>拷贝<code>%APPDATA%\Microsoft\Windows\Recent\AutomaticDestinations</code>至 运行 或 文件夹地址栏，回车；</li>
<li>删除其中文件<code>f01b4d95cf55d32a.automaticDestinations-ms</code>。</li>
</ol>
<p>DONE，整个世界都清净了。</p>
<hr>
<p>参考链接：<br>
<a href="https://blog.csdn.net/yin0hao/article/details/88052343">Windows 10 快速访问异常:无法取消固定,的解决方法</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[helm安装kube-prometheus-stack]]></title>
        <id>https://tomhht.github.io/post/an-zhuang-prometheus-operator/</id>
        <link href="https://tomhht.github.io/post/an-zhuang-prometheus-operator/">
        </link>
        <updated>2020-09-13T09:45:34.000Z</updated>
        <summary type="html"><![CDATA[<p>众所周知的原因搞得安装起来太复杂了，并且helm的prometheus operator的chart项目还迁移过，更有点云里雾里。</p>
]]></summary>
        <content type="html"><![CDATA[<p>众所周知的原因搞得安装起来太复杂了，并且helm的prometheus operator的chart项目还迁移过，更有点云里雾里。</p>
<!-- more -->
<h1 id="创建专用namespace">创建专用namespace</h1>
<p>先创建个namespace来用。</p>
<pre><code>kubectl create namespace monitoring
</code></pre>
<h1 id="单独安装crd">单独安装CRD</h1>
<p>CRD就是Custom Resource Definition。<br>
本来应该是<code>helm install kube-prometheus-stack</code>的时候就能装，but as you know..<br>
通过科学上网，先下载yaml再说：</p>
<pre><code># 整齐点儿
mkdir -p ~/helm/prometheus-operator
cd ~/helm/prometheus-operator

# 下载yaml
curl -x http://192.168.127.1:58591 -fsSLO https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml
curl -x http://192.168.127.1:58591 -fsSLO https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml
curl -x http://192.168.127.1:58591 -fsSLO https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml
curl -x http://192.168.127.1:58591 -fsSLO https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
curl -x http://192.168.127.1:58591 -fsSLO https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
curl -x http://192.168.127.1:58591 -fsSLO https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
curl -x http://192.168.127.1:58591 -fsSLO https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml

# 安装
kubectl apply -f .
</code></pre>
<h1 id="安装-kube-prometheus-stack">安装 kube-prometheus-stack</h1>
<p>kube-prometheus-stack基于prometheus-operator制作，并集成了依赖</p>
<ul>
<li>stable/kube-state-metrics</li>
<li>stable/prometheus-node-exporter</li>
<li>grafana/grafana</li>
</ul>
<pre><code># 安装
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --set prometheusOperator.createCustomResource=false -n monitoring
# 查看安装情况
kubectl -n monitoring get deployments
NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
kube-prometheus-stack-grafana              1/1     1            1           4h30m
kube-prometheus-stack-kube-state-metrics   1/1     1            1           4h30m
kube-prometheus-stack-operator             1/1     1            1           4h30m

kubectl -n monitoring get pods -owide
NAME                                                        READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
alertmanager-kube-prometheus-stack-alertmanager-0           2/2     Running   2          4h14m   10.244.1.32       mini-ubuntu    &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-grafana-8b85d667c-5w8sn               2/2     Running   4          4h32m   10.244.2.23       mini-centos    &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-kube-state-metrics-5cf575d8f8-6m7jt   1/1     Running   1          4h32m   10.244.1.33       mini-ubuntu    &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-operator-65fbd96bdb-9zhmz             2/2     Running   6          4h32m   10.244.2.21       mini-centos    &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-prometheus-node-exporter-68sxv        1/1     Running   2          4h32m   192.168.127.133   mini-centos    &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-prometheus-node-exporter-864w9        1/1     Running   1          4h32m   192.168.127.132   ubuntuserver   &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-prometheus-node-exporter-nr274        1/1     Running   1          4h32m   192.168.127.134   mini-ubuntu    &lt;none&gt;           &lt;none&gt;
prometheus-kube-prometheus-stack-prometheus-0               3/3     Running   6          4h14m   10.244.2.22       mini-centos    &lt;none&gt;           &lt;none&gt;
</code></pre>
<blockquote>
<p>遇到问题往后看。</p>
</blockquote>
<h1 id="安装过程的troubleshooting">安装过程的Troubleshooting</h1>
<p>安装过程各种报错，当然主要是镜像pull不下来，尤其是<code>quay.io</code>的镜像。(我的挺奇怪，居然自己pull成功了几个<code>quay.io</code>)</p>
<h2 id="排查问题">排查问题</h2>
<pre><code>kubectl -n monitoring describe pods prometheus-kube-prometheus-stack-prometheus-0

Events:
  Type     Reason   Age                    From                  Message
  ----     ------   ----                   ----                  -------
  Normal   Pulling  60m (x4 over 83m)      kubelet, mini-centos  Pulling image &quot;quay.io/coreos/prometheus-config-reloader:v0.38.1&quot;
  Warning  Failed   58m (x4 over 79m)      kubelet, mini-centos  Failed to pull image &quot;quay.io/coreos/prometheus-config-reloader:v0.38.1&quot;: rpc error: code = Unknown desc = context canceled
  Warning  Failed   58m (x4 over 79m)      kubelet, mini-centos  Error: ErrImagePull
  Normal   Pulling  58m (x5 over 100m)     kubelet, mini-centos  Pulling image &quot;quay.io/prometheus/prometheus:v2.18.2&quot;
  Normal   Pulled   47m                    kubelet, mini-centos  Successfully pulled image &quot;quay.io/coreos/prometheus-config-reloader:v0.38.1&quot; in 1m11.597994512s
  Warning  Failed   22m (x11 over 83m)     kubelet, mini-centos  Error: ErrImagePull
  Normal   BackOff  13m (x130 over 56m)    kubelet, mini-centos  Back-off pulling image &quot;quay.io/prometheus/prometheus:v2.18.2&quot;
  Warning  Failed   8m15s (x145 over 56m)  kubelet, mini-centos  Error: ImagePullBackOff
  Warning  Failed   3m13s (x14 over 83m)   kubelet, mini-centos  Failed to pull image &quot;quay.io/prometheus/prometheus:v2.18.2&quot;: rpc error: code = Unknown desc = context canceled
</code></pre>
<h2 id="解决">解决</h2>
<p><code>google prometheus v2.18.2</code>居然发现<code>docker hub</code>里有，于是在<code>mini-centos</code>主机上：</p>
<pre><code>docker pull prom/prometheus:v2.18.2
v2.18.2: Pulling from prom/prometheus
0f8c40e1270f: Already exists 
626a2a3fee8c: Already exists 
dde61fbb486b: Already exists 
22b936665674: Downloading [=================================================&gt; ]  20.66MB/20.76MB
b9cb37b79bc0: Download complete 
a14008f52cd5: Download complete 
c89901a55493: Waiting 
</code></pre>
<p>就定这儿了，遂祭起重启docker大法：</p>
<pre><code>sudo systemctl restart docker
</code></pre>
<p>重启完后，接着：</p>
<pre><code>docker pull prom/prometheus:v2.18.2
v2.18.2: Pulling from prom/prometheus
0f8c40e1270f: Already exists 
626a2a3fee8c: Already exists 
dde61fbb486b: Already exists 
22b936665674: Pull complete 
b9cb37b79bc0: Pull complete 
a14008f52cd5: Pull complete 
c89901a55493: Pull complete 
6caddd047c54: Pull complete 
ddd1779460e5: Pull complete 
f65cc1a7f25d: Pull complete 
4be7bc9429a7: Pull complete 
2d296d78b122: Pull complete 
Digest: sha256:4d3303d1eb424e345cf48969bb7575d4d58472ad783ac41ea07fba92686f7ef5
Status: Downloaded newer image for prom/prometheus:v2.18.2
docker.io/prom/prometheus:v2.18.2
</code></pre>
<p>再检查：</p>
<pre><code>kubectl -n monitoring get pods -owide
NAME                                                        READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
alertmanager-kube-prometheus-stack-alertmanager-0           2/2     Running   2          4h14m   10.244.1.32       mini-ubuntu    &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-grafana-8b85d667c-5w8sn               2/2     Running   4          4h32m   10.244.2.23       mini-centos    &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-kube-state-metrics-5cf575d8f8-6m7jt   1/1     Running   1          4h32m   10.244.1.33       mini-ubuntu    &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-operator-65fbd96bdb-9zhmz             2/2     Running   6          4h32m   10.244.2.21       mini-centos    &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-prometheus-node-exporter-68sxv        1/1     Running   2          4h32m   192.168.127.133   mini-centos    &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-prometheus-node-exporter-864w9        1/1     Running   1          4h32m   192.168.127.132   ubuntuserver   &lt;none&gt;           &lt;none&gt;
kube-prometheus-stack-prometheus-node-exporter-nr274        1/1     Running   1          4h32m   192.168.127.134   mini-ubuntu    &lt;none&gt;           &lt;none&gt;
prometheus-kube-prometheus-stack-prometheus-0               3/3     Running   6          4h14m   10.244.2.22       mini-centos    &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>全都ready了。<br>
如有其他pull问题，应该可以如法炮制。</p>
<h1 id="暴露服务">暴露服务</h1>
<h2 id="查看服务">查看服务</h2>
<pre><code>kubectl -n monitoring get svc
NAME                                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
alertmanager-operated                            ClusterIP   None             &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   4h16m
kube-prometheus-stack-alertmanager               ClusterIP   10.101.245.140   &lt;none&gt;        9093/TCP                     4h34m
kube-prometheus-stack-grafana                    ClusterIP   10.106.193.98    &lt;none&gt;        80/TCP                       4h34m
kube-prometheus-stack-kube-state-metrics         ClusterIP   10.108.208.70    &lt;none&gt;        8080/TCP                     4h34m
kube-prometheus-stack-operator                   ClusterIP   10.110.217.187   &lt;none&gt;        8080/TCP,443/TCP             4h34m
kube-prometheus-stack-prometheus                 ClusterIP   10.104.189.224   &lt;none&gt;        9090/TCP                     4h34m
kube-prometheus-stack-prometheus-node-exporter   ClusterIP   10.102.189.12    &lt;none&gt;        9100/TCP                     4h34m
prometheus-operated                              ClusterIP   None             &lt;none&gt;        9090/TCP                     4h16m
</code></pre>
<h2 id="改nodeport">改NodePort</h2>
<p>把下面服务的type改成NodePort：</p>
<pre><code>kubectl -n monitoring edit svc kube-prometheus-stack-prometheus
kubectl -n monitoring edit svc kube-prometheus-stack-alertmanager
kubectl -n monitoring edit svc kube-prometheus-stack-grafana

kubectl -n monitoring get svc
NAME                                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
alertmanager-operated                            ClusterIP   None             &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   4h51m
kube-prometheus-stack-alertmanager               NodePort    10.101.245.140   &lt;none&gt;        9093:31386/TCP               5h9m
kube-prometheus-stack-grafana                    NodePort    10.106.193.98    &lt;none&gt;        80:30835/TCP                 5h9m
kube-prometheus-stack-kube-state-metrics         ClusterIP   10.108.208.70    &lt;none&gt;        8080/TCP                     5h9m
kube-prometheus-stack-operator                   ClusterIP   10.110.217.187   &lt;none&gt;        8080/TCP,443/TCP             5h9m
kube-prometheus-stack-prometheus                 NodePort    10.104.189.224   &lt;none&gt;        9090:30098/TCP               5h9m
kube-prometheus-stack-prometheus-node-exporter   ClusterIP   10.102.189.12    &lt;none&gt;        9100/TCP                     5h9m
prometheus-operated                              ClusterIP   None             &lt;none&gt;        9090/TCP                     4h51m
</code></pre>
<h1 id="访问prometheus">访问prometheus</h1>
<p>打开http://192.168.127.132:30098/targets，一部分up一部分down，点Unhealthy<br>
<img src="https://tomhht.github.io/post-images/1600245194725.png" alt="unhealthy" loading="lazy"></p>
<h2 id="解决down的部分">解决down的部分</h2>
<h3 id="etcd的down">etcd的down</h3>
<blockquote>
<p>搞了1.5天，搜到的各种资料都无法适用，结合各家之言，再无数次尝试才勉强有结果。</p>
</blockquote>
<pre><code>sudo vim /etc/kubernetes/manifests/etcd.yaml
# 找到
- --listen-client-urls=https://127.0.0.1:2379,https://192.168.127.132:2379
# 修改为
- --listen-client-urls=https://0.0.0.0:2379,http://0.0.0.0:2379
# 重启全部container（实际上是停止命令，k8s会自动全部启动）
docker stop $(docker ps -q)
</code></pre>
<blockquote>
<p>挺奇葩的一点是，down变为up后，我又改回原配置，再重启全部container，依然保持up状态，存疑ing。</p>
</blockquote>
<h3 id="kube-scheduler的down">kube-scheduler的down</h3>
<pre><code>sudo vim /etc/kubernetes/manifests/kube-scheduler.yaml
//改为- --bind-address=0.0.0.0
//删行- --port=0
//稍等几秒即可
</code></pre>
<blockquote>
<p>这个要特别感谢度娘帮我搜到的<a href="https://llovewxm1314.blog.csdn.net/article/details/108458197">解决kubernetes:v1.18.6-1.19.0 get cs127.0.0.1 connection refused错误</a>。一个port你说你默认等于0干啥，你自己都unhealthy你说prometheus哪儿能监控得到。感谢原作者，n个小时后终于破案。</p>
</blockquote>
<h3 id="kube-controller-manager的down">kube-controller-manager的down</h3>
<p>修改套路同kube-scheduler。</p>
<pre><code>sudo vim /etc/kubernetes/manifests/kube-controller-manager.yaml
//改为- --bind-address=0.0.0.0
//删行- --port=0
//稍等几秒即可
</code></pre>
<h3 id="node-exporter的down">node-exporter的down</h3>
<pre><code>kubectl -n monitoring edit ds kube-prometheus-stack-prometheus-node-exporter
//删掉 hostNetwork: true 一行
//稍等几秒即可
//还没尝试helm给参数的方式，有机会试试override the hostNetwork setting using a values file with -f option
</code></pre>
<blockquote>
<p>特别感谢google来的<a href="https://vividcode.io/fix-context-deadline-exceeded-error-in-prometheus-operator/">Fix Context Deadline Exceeded Error in Prometheus Operator</a></p>
</blockquote>
<h3 id="kube-proxy的down尚未搞定">kube-proxy的down（尚未搞定）</h3>
<p>暂时跳过，快崩溃了（人），下面的方法用了根本不行，google bing baidu齐上阵，前前后后得搞了1天。好心的哪位，可否帮忙留言个正解？</p>
<pre><code># 可能用得到的指令
kubectl -n kube-system edit cm kube-proxy  //改metricsBindAddress: 为 0.0.0.0:10249
kubectl delete pod -l k8s-app=kube-proxy -n kube-system
// 按上面的操作，完全没效果，晕
sudo netstat -ntlp | grep kube
sudo ss -ntlp | grep kube
</code></pre>
<blockquote>
<p>这是第一个开始搞的，结果其他都搞定了，它还岿然不动。</p>
</blockquote>
<h1 id="grafana篇章">Grafana篇章</h1>
<p>kube-prometheus-stack里已经自带Grafana，因此无需安装。</p>
<h2 id="访问grafana">访问grafana</h2>
<pre><code>kubectl -n monitoring get svc //找到grafana的端口，然后浏览器访问http://IP:PORT
</code></pre>
<p><strong>用户名/密码是？</strong><br>
一堆说<code>admin/admin</code>的，然而这个环境不适用。</p>
<pre><code>用户名：admin
密码：prom-operator
</code></pre>
<blockquote>
<p>特别感谢<a href="https://devstack.in/2020/05/25/deploy-prometheus-operator-with-helm3-and-private-registry/">Deploy Prometheus Operator with Helm3 and Private Registry</a></p>
</blockquote>
<p><strong>漂亮</strong><br>
<img src="https://tomhht.github.io/post-images/1600249518889.png" alt="UI" loading="lazy"><br>
<strong>在Manage里随便选一个</strong><br>
<img src="https://tomhht.github.io/post-images/1600250887179.png" alt="magic" loading="lazy"><br>
<img src="https://tomhht.github.io/post-images/1600249787073.png" alt="gorgeous" loading="lazy"></p>
<h1 id="日志篇章efk">日志篇章EFK</h1>
<p>Elasticsearch+Fluentd+Kibana<br>
大部分参考<a href="https://www.qikqiak.com/post/install-efk-stack-on-k8s/">一文彻底搞定 EFK 日志收集</a>。</p>
<h2 id="几处调整">几处调整</h2>
<h3 id="elasticsearch-statefulsetyaml">elasticsearch-statefulset.yaml</h3>
<pre><code># 执行之前，给nodes打es=log标签
kubectl label nodes --all es=log

#  修改containers的image，因为docker.elastic.co的慢，pull容易失败
      image: elasticsearch:7.9.1

# 因为是测试，没有rook ceph，因此就替换volumn为emptydir
#  volumeClaimTemplates: 段落改为了：
      volumes:
      - name: data
        emptyDir: {}
</code></pre>
<blockquote>
<p>用kubectl -n logging get pods观察，发现statefulset的pod是one by one搞的。部署过程不快，看了下，elasticsearch的镜像700多MB。</p>
</blockquote>
<hr>
<p>参考链接：<br>
<a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack">https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack</a><br>
<a href="https://github.com/prometheus-operator/prometheus-operator">https://github.com/prometheus-operator/prometheus-operator</a><br>
<a href="https://github.com/prometheus-operator/kube-prometheus">https://github.com/prometheus-operator/kube-prometheus</a><br>
<a href="https://www.bookstack.cn/read/kubernetes-learning-0.2.0/docs-58.Prometheus%20Operator.md">https://www.bookstack.cn/read/kubernetes-learning-0.2.0/docs-58.Prometheus%20Operator.md</a><br>
<a href="https://www.qikqiak.com/post/prometheus-operator-monitor-etcd/">https://www.qikqiak.com/post/prometheus-operator-monitor-etcd/</a><br>
<a href="https://blog.fleeto.us/post/node-downtime/">https://blog.fleeto.us/post/node-downtime/</a><br>
<a href="https://llovewxm1314.blog.csdn.net/article/details/108458197">解决kubernetes:v1.18.6-1.19.0 get cs127.0.0.1 connection refused错误</a><br>
<a href="https://vividcode.io/fix-context-deadline-exceeded-error-in-prometheus-operator/">Fix Context Deadline Exceeded Error in Prometheus Operator</a><br>
<a href="https://devstack.in/2020/05/25/deploy-prometheus-operator-with-helm3-and-private-registry/">Deploy Prometheus Operator with Helm3 and Private Registry</a><br>
<a href="https://www.qikqiak.com/post/install-efk-stack-on-k8s/">一文彻底搞定 EFK 日志收集</a><br>
稍后阅读：<br>
<a href="https://www.qikqiak.com/post/k8s-hpa-usage/">Kubernetes HPA 使用详解</a><br>
<a href="https://blog.51cto.com/14143894/2438026">Kubernetes运维之使用Prometheus全方位监控K8S</a><br>
<a href="https://www.cnblogs.com/williamjie/p/10836799.html">Linux操作系统load average过高，kworker占用较多cpu</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[最小化安装centos和ubuntu之后的配置]]></title>
        <id>https://tomhht.github.io/post/zui-xiao-hua-an-zhuang-centos-hou-de-she-zhi/</id>
        <link href="https://tomhht.github.io/post/zui-xiao-hua-an-zhuang-centos-hou-de-she-zhi/">
        </link>
        <updated>2020-09-10T10:54:55.000Z</updated>
        <summary type="html"><![CDATA[<p>最小化安装centos 8后的设置。</p>
]]></summary>
        <content type="html"><![CDATA[<p>最小化安装centos 8后的设置。</p>
<!-- more -->
<h1 id="更换国内源">更换国内源</h1>
<p>参考<a href="/post/pei-zhi-ubuntu-he-centos-qing-hua-yuan">配置清华源</a></p>
<h1 id="安装epel">安装epel</h1>
<pre><code>sudo yum install epel-release
# 更换清华源
sudo sed -e 's!^metalink=!#metalink=!g' \
    -e 's!^#baseurl=!baseurl=!g' \
    -e 's!//download\.fedoraproject\.org/pub!//mirrors.tuna.tsinghua.edu.cn!g' \
    -e 's!http://mirrors\.tuna!https://mirrors.tuna!g' \
    -i /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel-testing.repo
sudo yum makecache
</code></pre>
<h1 id="更新软件">更新软件</h1>
<pre><code>yum check-update
sudo yum upgrade
</code></pre>
<h1 id="设置时区和ntp">设置时区和NTP</h1>
<pre><code>timedatectl set-timezone Asia/Shanghai
sudo yum install chrony
sudo systemctl enable --now chronyd
timedatectl status
</code></pre>
<h1 id="自动补全">自动补全</h1>
<pre><code>sudo yum install bash-completion
source ~/.bashrc
</code></pre>
<h1 id="配置vim">配置vim</h1>
<pre><code>sudo vim /etc/vim/vimrc.local
# 粘贴下面的内容进去
set showcmd
set showmatch
set ignorecase
set smartcase
set incsearch
set autowrite
set hidden
set nocompatible
set number
set autoindent
set smartindent
set tabstop=2
set shiftwidth=2
set autoread
set wildmenu
set expandtab
if has(&quot;vms&quot;)
set nobackup
else
set backup
endif
filetype indent on
filetype on
</code></pre>
<p>保存后，执行<code>source /etc/vim/vimrc</code>。</p>
<h1 id="安装wget">安装wget</h1>
<pre><code>sudo yum -y install wget
</code></pre>
<h1 id="其他有用的记录">其他有用的记录</h1>
<pre><code># 找程序所属的包
yum whatprovides netstat
</code></pre>
<hr>
<p>参考链接：<br>
<a href="https://www.cnblogs.com/fengdejiyixx/p/11592417.html">Centos和Ubuntu系统最小化安装基础命令</a><br>
<a href="https://linuxconfig.org/ubuntu-20-04-server-installation">Ubuntu 20.04 Server Installation</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用virsh删除kvm虚拟机]]></title>
        <id>https://tomhht.github.io/post/shi-yong-virsh-shan-chu-kvm-xu-ni-ji/</id>
        <link href="https://tomhht.github.io/post/shi-yong-virsh-shan-chu-kvm-xu-ni-ji/">
        </link>
        <updated>2020-09-10T06:21:16.000Z</updated>
        <summary type="html"><![CDATA[<p>发现libvirt/images下有个qcow2文件挺大，于是想用virsh安全删掉。</p>
]]></summary>
        <content type="html"><![CDATA[<p>发现libvirt/images下有个qcow2文件挺大，于是想用virsh安全删掉。</p>
<!-- more -->
<h1 id="列出虚拟机">列出虚拟机</h1>
<p>开始没用sudo，结果啥都没有，明明记得之前搞过俩呢。</p>
<pre><code>sudo virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     vm1                            shut off
 -     vm2                            shut off
</code></pre>
<h1 id="检查虚拟机配置">检查虚拟机配置</h1>
<pre><code>sudo virsh dumpxml vm1 | grep 'source file'
    &lt;source file='/var/lib/libvirt/images/cirros-0.5.1-x86_64-disk.img'/&gt;
    &lt;source file='/var/lib/libvirt/images/vm1.qcow2'/&gt; #要删这个
</code></pre>
<h1 id="编辑配置">编辑配置</h1>
<p>确保该虚拟机未运行。删除配置中挂载<code>vm1.qcow2</code>的段落。</p>
<pre><code>sudo virsh edit vm1
# 删除这段
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='qcow2'/&gt;
      &lt;source file='/var/lib/libvirt/images/vm1.qcow2'/&gt;
      &lt;target dev='hda' bus='ide'/&gt;
      &lt;address type='drive' controller='0' bus='0' target='0' unit='1'/&gt;
    &lt;/disk&gt; 
</code></pre>
<h1 id="删除文件">删除文件</h1>
<pre><code>sudo rm /var/lib/libvirt/images/vm1.qcow2
</code></pre>
<p>搞定。</p>
<hr>
<p>参考链接：<br>
<a href="https://huataihuang.gitbooks.io/cloud-atlas/content/virtual/kvm/startup/in_action/delete_a_running_vm_on_kvm.html">使用virsh删除运行的KVM VM · Cloud Atlas</a><br>
<a href="https://www.jianshu.com/p/5111134452a8">KVM磁盘扩容和添加</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[在Ubuntu上用snap安装Helm]]></title>
        <id>https://tomhht.github.io/post/zai-ubuntu-shang-yong-snap-an-zhuang-helm/</id>
        <link href="https://tomhht.github.io/post/zai-ubuntu-shang-yong-snap-an-zhuang-helm/">
        </link>
        <updated>2020-09-07T08:45:29.000Z</updated>
        <summary type="html"><![CDATA[<p>命令挺简单，就是速度很感人。</p>
]]></summary>
        <content type="html"><![CDATA[<p>命令挺简单，就是速度很感人。</p>
<!-- more -->
<h1 id="安装">安装</h1>
<pre><code>sudo snap install helm --classic
</code></pre>
<p>安装过慢的解决办法还没尝试，参考链接先记录下来了，先谢谢原作者。</p>
<blockquote>
<p>据官方说明，Helm 3已经不需要Tiller了，所以其他参考资料上安装Tiller的步骤直接跳过。</p>
</blockquote>
<h1 id="配置">配置</h1>
<h2 id="自动补全">自动补全</h2>
<pre><code>echo 'source &lt;(helm completion bash)' &gt;&gt;~/.bashrc
source ~/.bashrc #使配置立刻生效
</code></pre>
<hr>
<p>参考链接：<br>
<a href="https://helm.sh/">Helm官方</a><br>
<a href="https://kuricat.com/gist/snap-install-too-slow-zmbjy">snap install 过慢 的解决方法 @Kurisu</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s测试mysql挂载nfs遇到的问题]]></title>
        <id>https://tomhht.github.io/post/pei-zhi-nfs/</id>
        <link href="https://tomhht.github.io/post/pei-zhi-nfs/">
        </link>
        <updated>2020-09-06T07:50:41.000Z</updated>
        <summary type="html"><![CDATA[<p>学习nfs的配置，并测试k8s部署mysql挂载nfs的方式。</p>
]]></summary>
        <content type="html"><![CDATA[<p>学习nfs的配置，并测试k8s部署mysql挂载nfs的方式。</p>
<!-- more -->
<h1 id="配置nfs">配置nfs</h1>
<h2 id="ubuntu">Ubuntu</h2>
<p>先将参考链接放这儿。<a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-20-04">How To Set Up an NFS Mount on Ubuntu 20.04</a></p>
<h2 id="centos">CentOS</h2>
<p><a href="https://qizhanming.com/blog/2018/08/08/how-to-install-nfs-on-centos-7">CentOS 7 下 yum 安装和配置 NFS</a></p>
<h1 id="k8s测试遇到的问题">k8s测试遇到的问题</h1>
<h2 id="镜像pull不下来">镜像pull不下来</h2>
<p><strong>问题描述</strong><br>
测试mysql挂载nfs失败了n次，一开始是镜像pull不下来。<br>
<strong>解决</strong><br>
后来添加阿里云加速器后重启node上的docker <code>sudo systemctl restart docker.service</code>解决了pull的问题。</p>
<h2 id="pod遇到backoff的错误">pod遇到backoff的错误</h2>
<p><strong>问题描述</strong><br>
pull下来了还是运行失败，pod总是显示backoff <code>Back-off restarting failed container</code>的错误，通过<code>kubectl logs &lt;mysql pod的名字&gt;</code>发现<code>chown: changing ownership of '/var/lib/mysql/': Operation not permitted</code>错误。<br>
<strong>解决</strong><br>
nfs配置文件<code>/etc/exports</code>里，加上了<code>no_root_squash</code>的选项。</p>
<hr>
<p>参考链接：<br>
<a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-20-04">How To Set Up an NFS Mount on Ubuntu 20.04</a><br>
<a href="https://qizhanming.com/blog/2018/08/08/how-to-install-nfs-on-centos-7">CentOS 7 下 yum 安装和配置 NFS</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes busybox nslookup提示无法解析的问题]]></title>
        <id>https://tomhht.github.io/post/kubernetes-busybox-nslookup-ti-shi-wu-fa-jie-xi-de-wen-ti/</id>
        <link href="https://tomhht.github.io/post/kubernetes-busybox-nslookup-ti-shi-wu-fa-jie-xi-de-wen-ti/">
        </link>
        <updated>2020-09-05T07:43:38.000Z</updated>
        <summary type="html"><![CDATA[<p>使用默认版本的busybox会出现nslookup提示无法解析的问题。使用1.28.3版本即可解决。</p>
]]></summary>
        <content type="html"><![CDATA[<p>使用默认版本的busybox会出现nslookup提示无法解析的问题。使用1.28.3版本即可解决。</p>
<!-- more -->
<p>症状：</p>
<pre><code>kubectl run busybox --rm -it --image=busybox /bin/sh
/ # nslookup httpd-svc
Server:		10.96.0.10
Address:	10.96.0.10:53

** server can't find httpd-svc.default.svc.cluster.local: NXDOMAIN

*** Can't find httpd-svc.svc.cluster.local: No answer
*** Can't find httpd-svc.cluster.local: No answer
*** Can't find httpd-svc.default.svc.cluster.local: No answer
*** Can't find httpd-svc.svc.cluster.local: No answer
*** Can't find httpd-svc.cluster.local: No answer
</code></pre>
<p>解决：</p>
<pre><code>kubectl run busybox --rm -it --image=busybox:1.28.3 /bin/sh
If you don't see a command prompt, try pressing enter.
/ # nslookup httpd-svc
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      httpd-svc
Address 1: 10.108.142.200 httpd-svc.default.svc.cluster.local
</code></pre>
<hr>
<p>参考链接：<br>
<a href="https://www.cnblogs.com/vincenshen/p/9751193.html">https://www.cnblogs.com/vincenshen/p/9751193.html</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[第一次正式开车]]></title>
        <id>https://tomhht.github.io/post/di-yi-ci-zheng-shi-kai-che/</id>
        <link href="https://tomhht.github.io/post/di-yi-ci-zheng-shi-kai-che/">
        </link>
        <updated>2020-09-05T05:44:19.000Z</updated>
        <summary type="html"><![CDATA[<p>拿了本大半年了，昨天第一次正式开车。</p>
]]></summary>
        <content type="html"><![CDATA[<p>拿了本大半年了，昨天第一次正式开车。</p>
<!-- more -->
<p>太刺激了。画龙、剐蹭、油合离门，应有尽有。科三考了3次才过，考过后抽奖中了一个陪练机会，跟着教练开了趟易县（同练的还有其他学员）。这就是之前全部的开车经历了。<br /><br>
昨天因为要接侄子，学校在一个鸟不拉屎的地方，约不到车，所以硬着头皮把家里的车开出来了。首先找地方掉头，光顾着看后面，结果车头蹭人家墙上了，引得人家出来检查。<br /><br>
好不容易掉好头，跟别的车交汇，一闪躲，噌，又是一声，之前左边，这次右边。<br /><br>
继续前行，后来想想，居然当时心态没崩。<br /><br>
画龙前行，志玲姐姐忽然来一句右转立刻左转，我这路口到底怎么走啊，遂决定直接左转，进了左转车道等红灯，发现右转后走10米果然有左转路，WTF。左转车道硬着头皮左转完了，终于找到一个路口直接掉头，走上了康庄大道。<br /><br>
转完了突然道路宽阔了起来，路上也没几辆车，于是let's go，4档的干活，这感觉就是乌龟爬到了兔子身上。<br /><br>
到校后，等了几个钟头，中途又开着出去练练手，接上娃后返程的路途也算有惊无险，终于平安的运送回家。庆幸的是这一路没有倒车入库、侧方停车之类的技术活。万幸。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s集群跨节点pods无法ping通以及无法curl]]></title>
        <id>https://tomhht.github.io/post/k8s-ji-qun-kua-jie-dian-pods-wu-fa-ping-tong/</id>
        <link href="https://tomhht.github.io/post/k8s-ji-qun-kua-jie-dian-pods-wu-fa-ping-tong/">
        </link>
        <updated>2020-09-03T10:33:25.000Z</updated>
        <summary type="html"><![CDATA[<p>除了k8s要求的端口要开，worker node上面的<code>8472/udp（vxlan backend）</code>也得开。（尽管参考链接里明确的是worker node要开，但master似乎也得开，否则master里面的coredns无法为worker node服务）</p>
]]></summary>
        <content type="html"><![CDATA[<p>除了k8s要求的端口要开，worker node上面的<code>8472/udp（vxlan backend）</code>也得开。（尽管参考链接里明确的是worker node要开，但master似乎也得开，否则master里面的coredns无法为worker node服务）</p>
<!-- more -->
<p>用<code>nmcli</code>查看，发现flannel 用到的是<code>vxlan backend</code>，因此需要打开<code>8472/udp</code>。<br  /><br>
打开后，发现Ubuntu的node可以被ping通，同时如果上面跑着<code>httpd</code>，其他node也可以<code>curl</code>到内容；而CentOS只能被ping通，<code>curl 10.244.2.19</code>的时候会显示<code>curl: (7) Failed to connect to 10.244.2.19 port 80: No route to host</code>。<br /><br>
查了n多资料，几乎都是关闭防火墙，关闭后确实能解决，但我想这不是生产环境中的做法。最终在这位仁兄Louis He的博文<a href="https://www.cnblogs.com/Dev0ps/p/11401530.html">Kubernetes集群开启Firewall</a>中找到答案<code>add-masquerade</code>。同时也谢谢让我找到8472端口的仁兄<a href="https://www.codenong.com/39293441/">关于网络：Kubernetes集群所需的端口</a>。</p>
<pre><code># Ubuntu
sudo ufw allow 8472/udp #搞定

# CentOS
sudo firewall-cmd --zone=public --add-port=8472/udp --permanent
sudo firewall-cmd --reload #这步搞完只能被ping通，curl时是No route to host，头疼死了
sudo firewall-cmd --add-masquerade --permanent
sudo firewall-cmd --reload #这才把curl也打通了，我太难了
</code></pre>
<hr>
<p>参考链接：<br>
<a href="https://www.codenong.com/39293441/">https://www.codenong.com/39293441/</a><br>
<a href="https://www.cnblogs.com/Dev0ps/p/11401530.html">https://www.cnblogs.com/Dev0ps/p/11401530.html</a></p>
]]></content>
    </entry>
</feed>